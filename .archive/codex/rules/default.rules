prefix_rule(pattern=["/bin/bash", "-lc", "python3 - <<'PY'\nimport urllib.parse\nimport urllib.request\n\nquery = \"KS X 6101 2024 OWPML developer community\" \nurl = \"https://duckduckgo.com/html/?q=\" + urllib.parse.quote(query)\n\nreq = urllib.request.Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\nwith urllib.request.urlopen(req, timeout=15) as resp:\n    data = resp.read(20000)\nprint(data[:2000].decode(\"utf-8\", errors=\"ignore\"))\nPY"], decision="allow")
prefix_rule(pattern=["/bin/bash", "-lc", "python3 - <<'PY'\nimport re\nimport urllib.parse\nimport urllib.request\n\nquery = \"KS X 6101 2024 OWPML\" \nurl = \"https://duckduckgo.com/html/?q=\" + urllib.parse.quote(query)\nreq = urllib.request.Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\nwith urllib.request.urlopen(req, timeout=15) as resp:\n    html = resp.read().decode(\"utf-8\", errors=\"ignore\")\n\n# Extract result links\nlinks = re.findall(r\"href=\\\"(https?://[^\\\"]+)\\\"\", html)\n# Filter out duckduckgo links\nclean = []\nfor link in links:\n    if 'duckduckgo.com' in link:\n        continue\n    if link not in clean:\n        clean.append(link)\n\nprint(\"count\", len(clean))\nfor link in clean[:15]:\n    print(link)\nPY"], decision="allow")
prefix_rule(pattern=["/bin/bash", "-lc", "python3 - <<'PY'\nimport re\nimport urllib.parse\nimport urllib.request\n\nquery = \"KS X 6101 2024 OWPML\" \nurl = \"https://duckduckgo.com/html/?q=\" + urllib.parse.quote(query)\nreq = urllib.request.Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\nwith urllib.request.urlopen(req, timeout=15) as resp:\n    html = resp.read().decode(\"utf-8\", errors=\"ignore\")\n\n# Extract uddg param\nlinks = []\nfor m in re.findall(r\"uddg=([^&\\\"]+)\", html):\n    link = urllib.parse.unquote(m)\n    if link not in links:\n        links.append(link)\n\nprint(\"count\", len(links))\nfor link in links[:20]:\n    print(link)\nPY"], decision="allow")
prefix_rule(pattern=["/bin/bash", "-lc", "python3 - <<'PY'\nimport re\nimport urllib.parse\nimport urllib.request\n\nquery = \"HWPX OWPML KS X 6101 2024 site:github.com\" \nurl = \"https://duckduckgo.com/html/?q=\" + urllib.parse.quote(query)\nreq = urllib.request.Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\nwith urllib.request.urlopen(req, timeout=15) as resp:\n    html = resp.read().decode(\"utf-8\", errors=\"ignore\")\n\nlinks = []\nfor m in re.findall(r\"uddg=([^&\\\"]+)\", html):\n    link = urllib.parse.unquote(m)\n    if link not in links:\n        links.append(link)\n\nprint(\"count\", len(links))\nfor link in links[:15]:\n    print(link)\nPY"], decision="allow")
prefix_rule(pattern=["/bin/bash", "-lc", "python3 - <<'PY'\nimport urllib.request\n\nurls = [\n    \"https://raw.githubusercontent.com/hancom-io/hwpx-owpml-model/main/README.md\",\n    \"https://raw.githubusercontent.com/openhwp/openhwp/main/README.md\",\n    \"https://raw.githubusercontent.com/neolord0/hwpxlib/master/README.md\",\n]\n\nfor url in urls:\n    print('==', url)\n    try:\n        data = urllib.request.urlopen(url, timeout=15).read().decode('utf-8', errors='ignore')\n        # show lines with 2024 or KS X 6101 or OWPML\n        lines = [line for line in data.splitlines() if any(k in line for k in ['2024','KS X 6101','OWPML','HWPX'])]\n        print('\\n'.join(lines[:10]))\n    except Exception as e:\n        print('error', e)\n    print('---')\nPY"], decision="allow")
prefix_rule(pattern=["/bin/bash", "-lc", "python3 - <<'PY'\nimport urllib.request\n\nurl = \"https://raw.githubusercontent.com/openhwp/openhwp/main/docs/hwpx/README.md\"\nprint('==', url)\ntry:\n    data = urllib.request.urlopen(url, timeout=15).read().decode('utf-8', errors='ignore')\n    lines = [line for line in data.splitlines() if any(k in line for k in ['2024','KS X 6101','OWPML','HWPX'])]\n    print('\\n'.join(lines[:20]))\nexcept Exception as e:\n    print('error', e)\nPY"], decision="allow")
prefix_rule(pattern=["/bin/bash", "-lc", "python3 - <<'PY'\nimport urllib.request, json\n\nurl = \"https://api.github.com/repos/openhwp/openhwp/contents/docs\"\nreq = urllib.request.Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\nwith urllib.request.urlopen(req, timeout=15) as resp:\n    data = json.loads(resp.read().decode('utf-8'))\n\nfor item in data:\n    if 'hwpx' in item.get('name',''):\n        print(item)\nPY"], decision="allow")
prefix_rule(pattern=["/bin/bash", "-lc", "python3 - <<'PY'\nimport urllib.request, json\n\nurl = \"https://api.github.com/repos/openhwp/openhwp/contents/docs/hwpx?ref=main\"\nreq = urllib.request.Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\nwith urllib.request.urlopen(req, timeout=15) as resp:\n    data = json.loads(resp.read().decode('utf-8'))\n\nfor item in data:\n    print(item['name'], item['download_url'])\nPY"], decision="allow")
prefix_rule(pattern=["/bin/bash", "-lc", "python3 - <<'PY'\nimport urllib.request\nimport re\n\nurls = [\n    \"https://raw.githubusercontent.com/openhwp/openhwp/main/docs/hwpx/00-introduction.md\",\n    \"https://raw.githubusercontent.com/openhwp/openhwp/main/docs/hwpx/02-references.md\",\n    \"https://raw.githubusercontent.com/openhwp/openhwp/main/docs/hwpx/17-compatibility.md\",\n]\n\nfor url in urls:\n    print('==', url)\n    data = urllib.request.urlopen(url, timeout=15).read().decode('utf-8', errors='ignore')\n    hits = [line for line in data.splitlines() if re.search(r\"2024|KS X 6101\", line)]\n    print('\\n'.join(hits[:20]))\n    print('---')\nPY"], decision="allow")
prefix_rule(pattern=["/bin/bash", "-lc", "python3 - <<'PY'\nimport urllib.request\nimport re\n\nbase = \"https://raw.githubusercontent.com/openhwp/openhwp/main/docs/hwpx/\"\nfiles = [\n    \"00-introduction.md\",\"01-scope.md\",\"02-references.md\",\"03-terms.md\",\"04-conformance.md\",\n    \"05-relations.md\",\"06-schema-overview.md\",\"07-formats.md\",\"08-packaging.md\",\"09-header-schema.md\",\n    \"10-body-schema.md\",\"11-masterpage-schema.md\",\"12-history-schema.md\",\"13-version-schema.md\",\n    \"14-settings-schema.md\",\"15-encryption.md\",\"16-signature.md\",\"17-compatibility.md\",\n    \"appendix-a-purpose.md\",\"appendix-explanation.md\",\"appendix-i-formula.md\",\n]\n\nfor f in files:\n    url = base + f\n    try:\n        data = urllib.request.urlopen(url, timeout=15).read().decode('utf-8', errors='ignore')\n    except Exception:\n        continue\n    if re.search(r\"2024|KS X 6101:2024|KS X 6101\", data):\n        lines = [line for line in data.splitlines() if re.search(r\"2024|KS X 6101\", line)]\n        if lines:\n            print(f\"== {f}\")\n            print(\"\\n\".join(lines[:10]))\n            print('---')\nPY"], decision="allow")
prefix_rule(pattern=["/bin/bash", "-lc", "python3 - <<'PY'\nimport urllib.request\nimport re\n\nbase = \"https://raw.githubusercontent.com/openhwp/openhwp/main/docs/hwpx/\"\nfiles = [\n    \"00-introduction.md\",\"01-scope.md\",\"02-references.md\",\"03-terms.md\",\"04-conformance.md\",\n    \"05-relations.md\",\"06-schema-overview.md\",\"07-formats.md\",\"08-packaging.md\",\"09-header-schema.md\",\n    \"10-body-schema.md\",\"11-masterpage-schema.md\",\"12-history-schema.md\",\"13-version-schema.md\",\n    \"14-settings-schema.md\",\"15-encryption.md\",\"16-signature.md\",\"17-compatibility.md\",\n    \"appendix-a-purpose.md\",\"appendix-explanation.md\",\"appendix-i-formula.md\",\n]\n\nfor f in files:\n    url = base + f\n    try:\n        data = urllib.request.urlopen(url, timeout=15).read().decode('utf-8', errors='ignore')\n    except Exception:\n        continue\n    if re.search(r\"2024|KS X 6101\", data):\n        lines = [line for line in data.splitlines() if re.search(r\"2024|KS X 6101\", line)]\n        if lines:\n            print(f\"== {f}\")\n            print(\"\\n\".join(lines[:10]))\n            print('---')\nPY"], decision="allow")
prefix_rule(pattern=["/bin/bash", "-lc", "python3 - <<'PY'\nimport urllib.request\n\nurl = \"https://www.sharedit.co.kr/posts/31576\"\nreq = urllib.request.Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\ntry:\n    data = urllib.request.urlopen(req, timeout=15).read().decode('utf-8', errors='ignore')\n    # crude extract: lines containing HWPX/OWPML/KS X 6101/2024\n    keywords = [\"HWPX\", \"OWPML\", \"KS X 6101\", \"2024\", \"한글\"]\n    hits = [line.strip() for line in data.splitlines() if any(k in line for k in keywords)]\n    print(\"hits\", len(hits))\n    print(\"\\n\".join(hits[:10]))\nexcept Exception as e:\n    print('error', e)\nPY"], decision="allow")
prefix_rule(pattern=["/bin/bash", "-lc", "python3 - <<'PY'\nimport re\nimport urllib.parse\nimport urllib.request\n\nquery = \"HWPX OWPML KS X 6101 2024 tistory velog\" \nurl = \"https://duckduckgo.com/html/?q=\" + urllib.parse.quote(query)\nreq = urllib.request.Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\nwith urllib.request.urlopen(req, timeout=15) as resp:\n    html = resp.read().decode(\"utf-8\", errors=\"ignore\")\n\nlinks = []\nfor m in re.findall(r\"uddg=([^&\\\"]+)\", html):\n    link = urllib.parse.unquote(m)\n    if link not in links:\n        links.append(link)\n\nfor link in links[:20]:\n    print(link)\nPY"], decision="allow")
prefix_rule(pattern=["/bin/bash", "-lc", "python3 - <<'PY'\nimport urllib.request\nimport re\n\nurls = [\n    \"https://atthesea.tistory.com/171\",\n    \"https://kcrown.tistory.com/45\",\n]\n\nfor url in urls:\n    print('==', url)\n    req = urllib.request.Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n    try:\n        data = urllib.request.urlopen(req, timeout=15).read().decode('utf-8', errors='ignore')\n    except Exception as e:\n        print('error', e)\n        print('---')\n        continue\n    # simple keyword scan\n    keywords = [\"HWPX\", \"OWPML\", \"KS X 6101\", \"2024\", \"한글\", \"표준\"]\n    hits = [line.strip() for line in data.splitlines() if any(k in line for k in keywords)]\n    print(\"hits\", len(hits))\n    print(\"\\n\".join(hits[:10]))\n    print('---')\nPY"], decision="allow")
prefix_rule(pattern=["/bin/bash", "-lc", "python3 - <<'PY'\nimport re\nimport urllib.parse\nimport urllib.request\n\nquery = \"\\\"KS X 6101:2024\\\"\" \nurl = \"https://duckduckgo.com/html/?q=\" + urllib.parse.quote(query)\nreq = urllib.request.Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\nwith urllib.request.urlopen(req, timeout=15) as resp:\n    html = resp.read().decode(\"utf-8\", errors=\"ignore\")\n\nlinks = []\nfor m in re.findall(r\"uddg=([^&\\\"]+)\", html):\n    link = urllib.parse.unquote(m)\n    if link not in links:\n        links.append(link)\n\nfor link in links[:20]:\n    print(link)\nPY"], decision="allow")
prefix_rule(pattern=["/bin/bash", "-lc", "python3 - <<'PY'\nimport re\nimport urllib.parse\nimport urllib.request\n\nquery = \"OWPML 2024\" \nurl = \"https://duckduckgo.com/html/?q=\" + urllib.parse.quote(query)\nreq = urllib.request.Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\nwith urllib.request.urlopen(req, timeout=15) as resp:\n    html = resp.read().decode(\"utf-8\", errors=\"ignore\")\n\nlinks = []\nfor m in re.findall(r\"uddg=([^&\\\"]+)\", html):\n    link = urllib.parse.unquote(m)\n    if link not in links:\n        links.append(link)\n\nfor link in links[:20]:\n    print(link)\nPY"], decision="allow")
prefix_rule(pattern=["/bin/bash", "-lc", "python3 - <<'PY'\nimport urllib.request\n\nurls = [\n    \"https://raw.githubusercontent.com/HwpxIO/HwpxIO/main/README.md\",\n    \"https://raw.githubusercontent.com/HwpxIO/HwpxIO/main/docs/README.md\",\n]\n\nfor url in urls:\n    print('==', url)\n    try:\n        data = urllib.request.urlopen(url, timeout=15).read().decode('utf-8', errors='ignore')\n        lines = [line for line in data.splitlines() if any(k in line for k in ['2024','KS X 6101','OWPML','HWPX'])]\n        print('\\n'.join(lines[:10]))\n    except Exception as e:\n        print('error', e)\n    print('---')\nPY"], decision="allow")
prefix_rule(pattern=["/bin/bash", "-lc", "python3 - <<'PY'\nimport urllib.request, json\n\nurl = \"https://api.github.com/orgs/HwpxIO/repos\"\nreq = urllib.request.Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\nwith urllib.request.urlopen(req, timeout=15) as resp:\n    data = json.loads(resp.read().decode('utf-8'))\n\nfor repo in data[:20]:\n    print(repo['name'], repo['html_url'])\nPY"], decision="allow")
prefix_rule(pattern=["/bin/bash", "-lc", "python3 - <<'PY'\nimport urllib.request\n\nurl = \"https://raw.githubusercontent.com/HwpxIO/hwpx-owpml-model/main/README.md\"\nprint('==', url)\ntry:\n    data = urllib.request.urlopen(url, timeout=15).read().decode('utf-8', errors='ignore')\n    lines = [line for line in data.splitlines() if any(k in line for k in ['2024','KS X 6101','OWPML','HWPX'])]\n    print('\\n'.join(lines[:10]))\nexcept Exception as e:\n    print('error', e)\nPY"], decision="allow")
prefix_rule(pattern=["/bin/bash", "-lc", "python3 - <<'PY'\nimport urllib.request\n\nurl = \"https://raw.githubusercontent.com/HwpxIO/HwpxIO.github.io/main/index.html\"\nprint('==', url)\ntry:\n    data = urllib.request.urlopen(url, timeout=15).read().decode('utf-8', errors='ignore')\n    lines = [line for line in data.splitlines() if any(k in line for k in ['2024','KS X 6101','OWPML','HWPX'])]\n    print('\\n'.join(lines[:10]))\nexcept Exception as e:\n    print('error', e)\nPY"], decision="allow")
prefix_rule(pattern=["/bin/bash", "-lc", "python3 - <<'PY'\nimport urllib.request, json\n\nurl = \"https://api.github.com/search/code?q=%22KS%20X%206101%3A2024%22\"\nreq = urllib.request.Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\nwith urllib.request.urlopen(req, timeout=15) as resp:\n    data = json.loads(resp.read().decode('utf-8'))\n\nprint('total', data.get('total_count'))\nitems = data.get('items', [])\nfor item in items[:10]:\n    print(item['html_url'])\nPY"], decision="allow")
prefix_rule(pattern=["/bin/bash", "-lc", "python3 - <<'PY'\nimport urllib.request\n\nurls = [\n    \"https://www.antpedia.com/standard/1518050584.html\",\n    \"https://ndls.org.cn/standard/detail/ab1e1c0b9c48bd9ee60cf63e966f564c\",\n]\n\nfor url in urls:\n    print('==', url)\n    req = urllib.request.Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n    try:\n        data = urllib.request.urlopen(req, timeout=15).read().decode('utf-8', errors='ignore')\n    except Exception as e:\n        print('error', e)\n        print('---')\n        continue\n    keywords = [\"KS X 6101\", \"6101\", \"2024\", \"OWPML\"]\n    hits = [line.strip() for line in data.splitlines() if any(k in line for k in keywords)]\n    print(\"hits\", len(hits))\n    print(\"\\n\".join(hits[:10]))\n    print('---')\nPY"], decision="allow")
prefix_rule(pattern=["/bin/bash", "-lc", "python3 - <<'PY'\nimport re\nimport urllib.parse\nimport urllib.request\n\nqueries = [\n    \"HWPX sample file 2024 owpml\",\n    \"OWPML 2024 sample hwpx\",\n    \"HWPX sample hwpx file\",\n    \"site:github.com hwpx sample\",\n    \"site:github.com owpml 2024 hwpx\",\n]\n\nfor query in queries:\n    print('==', query)\n    url = \"https://duckduckgo.com/html/?q=\" + urllib.parse.quote(query)\n    req = urllib.request.Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n    with urllib.request.urlopen(req, timeout=15) as resp:\n        html = resp.read().decode(\"utf-8\", errors=\"ignore\")\n    links = []\n    for m in re.findall(r\"uddg=([^&\\\"]+)\", html):\n        link = urllib.parse.unquote(m)\n        if link not in links:\n            links.append(link)\n    for link in links[:10]:\n        print(link)\n    print('---')\nPY"], decision="allow")
prefix_rule(pattern=["/bin/bash", "-lc", "python3 - <<'PY'\nimport urllib.request, json\n\nrepos = [\n    (\"hancom-io\", \"hwpx-owpml-model\"),\n    (\"openhwp\", \"openhwp\"),\n    (\"airmang\", \"python-hwpx-owpml\"),\n    (\"neolord0\", \"hwpxlib\"),\n]\n\nfor owner, repo in repos:\n    print('==', owner, repo)\n    url = f\"https://api.github.com/repos/{owner}/{repo}/git/trees/main?recursive=1\"\n    req = urllib.request.Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n    try:\n        data = json.loads(urllib.request.urlopen(req, timeout=20).read().decode('utf-8'))\n    except Exception as e:\n        print('error', e)\n        continue\n    paths = [item['path'] for item in data.get('tree', []) if item['path'].lower().endswith('.hwpx')]\n    print('hwpx count', len(paths))\n    for p in paths[:20]:\n        print(' ', p)\n    print('---')\nPY"], decision="allow")
prefix_rule(pattern=["/bin/bash", "-lc", "python3 - <<'PY'\nimport urllib.request, json\n\nurl = \"https://api.github.com/repos/neolord0/hwpxlib\"\nreq = urllib.request.Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\nwith urllib.request.urlopen(req, timeout=15) as resp:\n    data = json.loads(resp.read().decode('utf-8'))\nprint('default_branch', data.get('default_branch'))\nPY"], decision="allow")
prefix_rule(pattern=["/bin/bash", "-lc", "python3 - <<'PY'\nimport os\nimport urllib.request\n\nsamples = {\n    \"hwpxlib_20240919_test.hwpx\": \"https://raw.githubusercontent.com/neolord0/hwpxlib/main/testFile/error/20240919/%ED%85%8C%EC%8A%A4%ED%8A%B8%EB%AC%B8%EC%84%9C.hwpx\",\n    \"hwpxlib_20241104_mot.hwpx\": \"https://raw.githubusercontent.com/neolord0/hwpxlib/main/testFile/error/20241104/mot.hwpx\",\n    \"hwpxlib_20240626_no_manifest.hwpx\": \"https://raw.githubusercontent.com/neolord0/hwpxlib/main/testFile/error/20240626/no_manifest.hwpx\",\n}\n\nout_dir = \"/home/palantir/hwpx/external_samples\"\nos.makedirs(out_dir, exist_ok=True)\n\nmeta_lines = []\nfor name, url in samples.items():\n    path = os.path.join(out_dir, name)\n    print('downloading', url)\n    data = urllib.request.urlopen(url, timeout=20).read()\n    with open(path, 'wb') as f:\n        f.write(data)\n    meta_lines.append(f\"{name}\\t{url}\")\n\nmeta_path = os.path.join(out_dir, \"SOURCES.tsv\")\nwith open(meta_path, 'w', encoding='utf-8') as f:\n    f.write(\"filename\\turl\\n\")\n    f.write(\"\\n\".join(meta_lines))\n\nprint('saved', out_dir)\nPY"], decision="allow")
prefix_rule(pattern=["/bin/bash", "-lc", "python3 - <<'PY'\nimport re\nimport urllib.parse\nimport urllib.request\n\nquery = '\"owpml.org/owpml/2024\"'\nurl = \"https://duckduckgo.com/html/?q=\" + urllib.parse.quote(query)\nreq = urllib.request.Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\nwith urllib.request.urlopen(req, timeout=15) as resp:\n    html = resp.read().decode('utf-8', errors='ignore')\n\nlinks = []\nfor m in re.findall(r\"uddg=([^&\\\"]+)\", html):\n    link = urllib.parse.unquote(m)\n    if link not in links:\n        links.append(link)\n\nfor link in links[:20]:\n    print(link)\nPY"], decision="allow")
prefix_rule(pattern=["/bin/bash", "-lc", "python3 - <<'PY'\nimport re\nimport urllib.parse\nimport urllib.request\n\nquery = 'owpml 2024 namespace'\nurl = \"https://duckduckgo.com/html/?q=\" + urllib.parse.quote(query)\nreq = urllib.request.Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\nwith urllib.request.urlopen(req, timeout=15) as resp:\n    html = resp.read().decode('utf-8', errors='ignore')\n\nlinks = []\nfor m in re.findall(r\"uddg=([^&\\\"]+)\", html):\n    link = urllib.parse.unquote(m)\n    if link not in links:\n        links.append(link)\n\nfor link in links[:20]:\n    print(link)\nPY"], decision="allow")
prefix_rule(pattern=["/bin/bash", "-lc", "python3 - <<'PY'\nimport urllib.request, json\n\nrepos = [\n    (\"HwpxIO\", \"hwpx-owpml-model\"),\n]\n\nfor owner, repo in repos:\n    print('==', owner, repo)\n    url = f\"https://api.github.com/repos/{owner}/{repo}/git/trees/main?recursive=1\"\n    req = urllib.request.Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n    try:\n        data = json.loads(urllib.request.urlopen(req, timeout=20).read().decode('utf-8'))\n    except Exception as e:\n        print('error', e)\n        continue\n    paths = [item['path'] for item in data.get('tree', []) if item['path'].lower().endswith('.hwpx')]\n    print('hwpx count', len(paths))\n    for p in paths[:20]:\n        print(' ', p)\n    print('---')\nPY"], decision="allow")
prefix_rule(pattern=["/bin/bash", "-lc", "python3 - <<'PY'\nimport re\nimport urllib.request\n\nurl = \"https://github.com/topics/hwpx\"\nreq = urllib.request.Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\nhtml = urllib.request.urlopen(req, timeout=15).read().decode('utf-8', errors='ignore')\n# Extract repo links\nrepos = re.findall(r\"/([A-Za-z0-9_.-]+/[A-Za-z0-9_.-]+)\\\"\", html)\n# Filter unique and plausible (skip topics)\nuniq = []\nfor r in repos:\n    if r.count('/') != 1:\n        continue\n    if r.startswith('topics/'):\n        continue\n    if r not in uniq:\n        uniq.append(r)\n\nprint('repos', len(uniq))\nfor r in uniq[:20]:\n    print(r)\nPY"], decision="allow")
prefix_rule(pattern=["/bin/bash", "-lc", "python3 - <<'PY'\nimport re\nimport urllib.request\n\nurl = \"https://github.com/topics/hwpx\"\nreq = urllib.request.Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\nhtml = urllib.request.urlopen(req, timeout=15).read().decode('utf-8', errors='ignore')\n\nlinks = re.findall(r\"href=\\\"/([A-Za-z0-9_.-]+/[A-Za-z0-9_.-]+)\\\"\", html)\nuniq = []\nfor r in links:\n    owner, repo = r.split('/')\n    if owner in {\"topics\", \"assets\", \"site\"}:\n        continue\n    if repo.endswith('.css') or repo.endswith('.js'):\n        continue\n    if r not in uniq:\n        uniq.append(r)\n\nprint('repos', len(uniq))\nfor r in uniq[:30]:\n    print(r)\nPY"], decision="allow")
prefix_rule(pattern=["/bin/bash", "-lc", "python3 - <<'PY'\nimport urllib.request, json\n\nrepos = [\n    \"openhwp/openhwp\",\n    \"hancom-io/hwpx-owpml-model\",\n    \"showjihyun/agentrag-v1\",\n    \"hancom-io/metatag-ex\",\n    \"nonbanana/pyhwpx\",\n    \"heosd/hwpx\",\n    \"KimYoungNo/Windows_HwpSecurityModule_Register\",\n    \"damulhan/hwp_extract\",\n    \"beskep/hwp-preview.yazi\",\n    \"j4docsearch/j4docsearch.github.io\",\n]\n\nfor repo in repos:\n    owner, name = repo.split('/')\n    print('==', repo)\n    url = f\"https://api.github.com/repos/{owner}/{name}/git/trees/main?recursive=1\"\n    req = urllib.request.Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n    try:\n        data = json.loads(urllib.request.urlopen(req, timeout=20).read().decode('utf-8'))\n    except Exception as e:\n        print('error', e)\n        continue\n    paths = [item['path'] for item in data.get('tree', []) if item['path'].lower().endswith('.hwpx')]\n    print('hwpx count', len(paths))\n    for p in paths[:20]:\n        print(' ', p)\n    print('---')\nPY"], decision="allow")
prefix_rule(pattern=["/bin/bash", "-lc", "python3 - <<'PY'\nimport os\nimport urllib.request\n\nsamples = {\n    \"pyhwpx_star_toxic.hwpx\": \"https://raw.githubusercontent.com/nonbanana/pyhwpx/main/%5B%EB%B3%84%ED%91%9C%5D%20%EC%9C%A0%EB%8F%85%EB%AC%BC%EC%A7%88(%EC%A0%9C3%EC%A1%B0%20%EA%B4%80%EB%A0%A8)(%EC%9C%A0%EB%8F%85%EB%AC%BC%EC%A7%88%EC%9D%98%20%EC%A7%80%EC%A0%95%EA%B3%A0%EC%8B%9C).hwpx\",\n    \"hwp_extract_multipara.hwpx\": \"https://raw.githubusercontent.com/damulhan/hwp_extract/main/sample/multipara.hwpx\",\n    \"hwp_extract_hello.hwpx\": \"https://raw.githubusercontent.com/damulhan/hwp_extract/main/sample/%EC%95%88%EB%85%95%ED%95%98%EC%84%B8%EC%9A%94.hwpx\",\n}\n\nout_dir = \"/home/palantir/hwpx/external_samples\"\nos.makedirs(out_dir, exist_ok=True)\n\nmeta_path = os.path.join(out_dir, \"SOURCES.tsv\")\nmeta_lines = []\nif os.path.exists(meta_path):\n    with open(meta_path, 'r', encoding='utf-8') as f:\n        meta_lines = [line.strip() for line in f if line.strip()]\n\nexisting = {line.split('\\t')[0] for line in meta_lines[1:]} if meta_lines else set()\n\nnew_lines = []\nfor name, url in samples.items():\n    if name in existing:\n        continue\n    path = os.path.join(out_dir, name)\n    print('downloading', url)\n    data = urllib.request.urlopen(url, timeout=20).read()\n    with open(path, 'wb') as f:\n        f.write(data)\n    new_lines.append(f\"{name}\\t{url}\")\n\nif not meta_lines:\n    meta_lines = [\"filename\\turl\"]\n\nmeta_lines.extend(new_lines)\n\nwith open(meta_path, 'w', encoding='utf-8') as f:\n    f.write(\"\\n\".join(meta_lines))\n\nprint('saved', out_dir)\nPY"], decision="allow")
