"""
Orion ODA v3.0 - LLM Configuration Module

LLM-Agnostic provider configuration supporting:
- Claude Code (native CLI integration)
- Antigravity (Gemini backend)
- Ollama (local models)
- OpenAI-compatible APIs

Environment Variables:
    ORION_LLM_PROVIDER: Provider selection (claude-code|antigravity|ollama|openai)
    ORION_LLM_BASE_URL: API base URL
    ORION_LLM_API_KEY: API key (not required for Claude Code or Ollama)
    ORION_LLM_MODEL: Model name
"""
from __future__ import annotations

import json
import os
import platform
from dataclasses import dataclass
from enum import Enum
from typing import Any, Dict, Optional


class LLMProviderType(str, Enum):
    """
    Supported LLM provider types.

    This enum ensures LLM-agnostic design by explicitly defining
    all supported providers. Each provider has its own configuration
    requirements and capabilities.
    """
    CLAUDE_CODE = "claude-code"      # Native Claude Code CLI (no API key needed)
    ANTIGRAVITY = "antigravity"      # Gemini backend via Antigravity
    OLLAMA = "ollama"                # Local Ollama models
    OPENAI = "openai"                # OpenAI-compatible APIs

    @classmethod
    def from_string(cls, value: str) -> "LLMProviderType":
        """Parse provider type from string with fallback."""
        normalized = value.lower().strip().replace("-", "_").replace(" ", "_")
        mapping = {
            "claude_code": cls.CLAUDE_CODE,
            "claude": cls.CLAUDE_CODE,
            "antigravity": cls.ANTIGRAVITY,
            "gemini": cls.ANTIGRAVITY,
            "ollama": cls.OLLAMA,
            "local": cls.OLLAMA,
            "openai": cls.OPENAI,
            "openai_compatible": cls.OPENAI,
        }
        return mapping.get(normalized, cls.OPENAI)


def _get_default_workspace_root() -> str:
    if platform.system() == "Windows":
        return os.environ.get("USERPROFILE", os.path.expanduser("~"))
    return "/home/palantir"


WORKSPACE_ROOT = os.environ.get("ORION_WORKSPACE_ROOT", _get_default_workspace_root())
ANTIGRAVITY_MCP_CONFIG_PATH = os.path.join(WORKSPACE_ROOT, ".gemini", "antigravity", "mcp_config.json")


@dataclass(frozen=True)
class LLMBackendConfig:
    provider: str
    base_url: str
    api_key: str
    model: str


def _read_json(path: str) -> Dict[str, Any]:
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
    if not isinstance(data, dict):
        raise ValueError(f"Expected JSON object in {path}")
    return data


def _load_antigravity_env_from_mcp(path: str = ANTIGRAVITY_MCP_CONFIG_PATH) -> Dict[str, str]:
    if not os.path.exists(path):
        return {}
    config = _read_json(path)
    servers = config.get("mcpServers", {})
    if not isinstance(servers, dict):
        return {}

    env: Dict[str, str] = {}
    for server in servers.values():
        if not isinstance(server, dict):
            continue
        server_env = server.get("env")
        if not isinstance(server_env, dict):
            continue
        for key in (
            "ANTIGRAVITY_LLM_BASE_URL",
            "ANTIGRAVITY_LLM_API_KEY",
            "ANTIGRAVITY_LLM_MODEL",
        ):
            if key in server_env:
                env[key] = str(server_env[key])
    return env


def _env_or_fallback(key: str, fallback: str) -> str:
    value = os.environ.get(key)
    return value if value else fallback


def load_llm_config() -> LLMBackendConfig:
    mcp_env = _load_antigravity_env_from_mcp()

    provider = os.environ.get("ORION_LLM_PROVIDER")
    if not provider:
        provider = "antigravity" if (
            os.environ.get("ANTIGRAVITY_LLM_BASE_URL") or mcp_env.get("ANTIGRAVITY_LLM_BASE_URL")
        ) else "openai-compatible"

    if provider == "antigravity":
        base_url = _env_or_fallback(
            "ANTIGRAVITY_LLM_BASE_URL",
            mcp_env.get("ANTIGRAVITY_LLM_BASE_URL", ""),
        )
        api_key = _env_or_fallback(
            "ANTIGRAVITY_LLM_API_KEY",
            mcp_env.get("ANTIGRAVITY_LLM_API_KEY", "antigravity"),
        )
        model = _env_or_fallback(
            "ANTIGRAVITY_LLM_MODEL",
            mcp_env.get("ANTIGRAVITY_LLM_MODEL", "gemini-3.0-pro"),
        )
        if not base_url:
            base_url = _env_or_fallback("ORION_LLM_BASE_URL", "http://localhost:11434/v1")
    else:
        base_url = _env_or_fallback("ORION_LLM_BASE_URL", "http://localhost:11434/v1")
        api_key = _env_or_fallback("ORION_LLM_API_KEY", "ollama")
        model = _env_or_fallback("ORION_LLM_MODEL", "llama3.2")

    return LLMBackendConfig(
        provider=provider,
        base_url=base_url,
        api_key=api_key,
        model=model,
    )


def get_default_model() -> str:
    return load_llm_config().model
