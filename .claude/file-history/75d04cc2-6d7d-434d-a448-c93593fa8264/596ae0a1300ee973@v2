"""
Orion ODA V3 - Semantic OS Kernel
=================================
Runtime kernel for processing cognitive tasks and proposals.

Performance Optimizations (ODA-RISK-013):
- Parallel proposal execution with configurable concurrency
- Batch processing to reduce DB round-trips
- Caching for frequently accessed data
"""
import asyncio
import os
import logging
from typing import Optional, List

from lib.oda.ontology.client import FoundryClient
from lib.oda.llm.instructor_client import InstructorClient
from lib.oda.relay.queue import RelayQueue
from lib.oda.ontology.storage import ProposalRepository, initialize_database
from lib.oda.ontology.objects.proposal import Proposal, ProposalStatus
from lib.oda.ontology.actions import action_registry, GovernanceEngine, ActionContext
from lib.oda.ontology.plan import Plan
from lib.oda.runtime.marshaler import ToolMarshaler
from lib.oda.aip_logic.engine import LogicEngine  # [AIP Logic Integration]

# Configure Logging
logging.basicConfig(level=logging.INFO, format="[%(name)s] %(message)s")
logger = logging.getLogger("Kernel")

class OrionRuntime:
    """
    The V3 Semantic OS Kernel (Generic Ontology Engine).

    Principles:
    1. Schema is Law: Uses `Plan` Pydantic model.
    2. Registry Supremacy: Actions via `ActionRegistry`.
    3. Metadata Governance: Policy via `GovernanceEngine`.
    4. Deterministic AI: Uses `instructor` for strict JSON outputs.
    5. Logic Injection: Centralized AI Logic Engine.

    Performance (ODA-RISK-013):
    - Configurable concurrency for proposal execution
    - Semaphore-based rate limiting
    - Batch processing for efficiency
    """
    def __init__(self):
        self.llm = InstructorClient()
        self.logic_engine = LogicEngine(self.llm)  # [DI] Centralized Logic Engine
        self.relay = RelayQueue()
        self.governance = GovernanceEngine(action_registry)
        self.marshaler = ToolMarshaler(action_registry)
        self.running = True
        self.repo: Optional[ProposalRepository] = None

        # P1.2: Status counters (IndyDevDan status line pattern)
        self._pending_count = 0
        self._executed_count = 0
        self._blocked_count = 0

        # ODA-RISK-013: Performance configuration
        self._max_concurrent_proposals = int(os.environ.get("ORION_MAX_CONCURRENT_PROPOSALS", "5"))
        self._proposal_semaphore: Optional[asyncio.Semaphore] = None
        self._batch_size = int(os.environ.get("ORION_PROPOSAL_BATCH_SIZE", "10"))

        logger.info("Semantic OS Kernel Booting... (Mode: Enterprise Async)")

    async def start(self):
        # Initialize Persistence Layer
        db = await initialize_database()
        self.repo = ProposalRepository(db)

        # ODA-RISK-013: Initialize concurrency control
        self._proposal_semaphore = asyncio.Semaphore(self._max_concurrent_proposals)

        logger.info(f"Online. Waiting for Semantic Signals... (concurrency={self._max_concurrent_proposals})")

        # P3-03: Configurable poll interval (DIA v2.1 C5 compliant)
        poll_interval = float(os.environ.get("ORION_POLL_INTERVAL", "1.0"))

        while self.running:
            # 1. Check Relay Queue (Cognitive Tasks) - Non-blocking async call
            task_payload = await self.relay.dequeue_async()
            if task_payload:
                logger.info(f"Processing Relay Task: {task_payload.get('id', 'unknown')}")
                await self._process_task_cognitive(task_payload)

            # 2. Check Approved Proposals (Execution Worker) - Now parallel
            await self._process_approved_proposals()

            await asyncio.sleep(poll_interval)

    async def _process_approved_proposals(self):
        """
        Execute proposals that have been approved.

        ODA-RISK-013: Uses parallel execution with semaphore-based rate limiting.
        """
        if not self.repo:
            return

        # ODA-RISK-013: Batch fetch with DB-level limit to prevent memory issues
        batch = await self.repo.find_by_status(ProposalStatus.APPROVED, limit=self._batch_size)
        if not batch:
            return

        logger.info(f"Processing batch of {len(batch)} approved proposals.")

        # ODA-RISK-013: Parallel execution with semaphore
        tasks = [self._execute_single_proposal(p) for p in batch]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # Log summary
        success_count = sum(1 for r in results if r is True)
        fail_count = sum(1 for r in results if r is False or isinstance(r, Exception))
        if success_count or fail_count:
            logger.info(f"Batch complete: {success_count} succeeded, {fail_count} failed")

    async def _execute_single_proposal(self, p: Proposal) -> bool:
        """
        Execute a single proposal with semaphore-based concurrency control.

        ODA-RISK-013: Rate-limited execution to prevent resource exhaustion.
        """
        async with self._proposal_semaphore:
            logger.info(f"üöÄ Executing Proposal {p.id} ({p.action_type})...")
            try:
                # Execute via Marshaler
                result = await self.marshaler.execute_action(
                    action_name=p.action_type,
                    params=p.payload,
                    context=ActionContext(actor_id=p.reviewed_by or "system")
                )

                await self.repo.execute(
                    p.id,
                    executor_id="kernel",
                    result=result.to_dict()
                )
                self._executed_count += 1
                logger.info(f"‚úÖ Execution verified for {p.id}")
                return True
            except Exception as e:
                logger.error(f"‚ùå Execution Failed for {p.id}: {e}")
                return False

    def shutdown(self):
        self.running = False
        logger.info("Kernel shutting down.")

    async def _process_task_cognitive(self, task_payload):
        """
        Cognitive Consumption: LLM Analysis -> Ontology Creation.
        Uses Instructor for reliable Plan parsing.
        """
        prompt = task_payload['prompt']
        logger.info(f"üß† Thinking... (Analyzing: '{prompt[:30]}...')")
        
        try:
            # 1. Ask LLM for Plan using Instructor (Schema is Law)
            # No manual JSON parsing needed here. Instructor guarantees Pydantic logic.
            # Running in ThreadPool if call is blocking, or using async client if implemented under hood.
            # Assuming InstructorClient wraps async or we run in executor.
            
            # Note: The InstructorClient.generate_plan is now ASYNC (ODA Compliant).
            # We await it directly.
            plan = await self.llm.generate_plan(prompt)
            
            logger.info(f"üêõ Plan Parsed Successfully: {len(plan.jobs)} jobs")

            # 2. Iterate and Dispatch
            for job in plan.jobs:
                logger.info(f"   Processing Job: {job.title} [{job.action_type}]")
                
                # A. Governance Check (P1.1: PolicyResult with reason)
                policy = self.governance.check_execution_policy(job.action_type, job.params)
                
                if policy.is_blocked():
                    self._blocked_count += 1
                    logger.error(f"   ‚õî BLOCKED: {policy.reason}")
                    continue
                
                if policy.decision == "REQUIRE_PROPOSAL":
                    if self.repo:
                        proposal = Proposal(
                            action_type=job.action_type,
                            payload=job.params,
                            created_by='kernel_ai',
                            priority=job.priority
                        )
                        # save now uses Async ORM
                        await self.repo.save(proposal, actor_id="kernel_ai")
                        self._pending_count += 1
                        logger.info(f"   üõ°Ô∏è  Proposal Created: {proposal.id} ({policy.reason})")
                    else:
                        logger.error("   ‚ùå Repo unavailable, cannot create proposal.")
                        
                elif policy.is_allowed():
                    # Instant Execution
                    logger.info(f"   ‚ö° Executing Immediately: {job.action_type}")
                    result = await self.marshaler.execute_action(
                        action_name=job.action_type,
                        params=job.params,
                        context=ActionContext.system()
                    )
                    if result.success:
                        self._executed_count += 1
                    else:
                        logger.error(f"      ‚ùå Immediate Execution Failed: {result.error}")

        except Exception as e:
            logger.error(f"‚ùå Cognitive Processing Failed: {e}")

if __name__ == "__main__":
    runtime = OrionRuntime()
    try:
        asyncio.run(runtime.start())
    except KeyboardInterrupt:
        runtime.shutdown()
