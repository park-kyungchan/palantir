---
name: ask
description: |
  Prompt Assistant를 호출하여 요구사항을 명확화하고 최적의 기능을 추천받습니다.
  **[V2.1.10] L2Synthesizer + PromptTemplateBuilder + Context Budget 통합**
  **[V2.1.10] AskOutput 스키마로 구조화된 JSON 출력 보장**
allowed-tools: Read, Grep, Glob, AskUserQuestion, Task, WebSearch, TodoWrite

# V2.1.x Features
v21x_features:
  context_budget_manager: true    # Pre-delegation context check
  progressive_disclosure: true    # L1 headline + L2 on-demand
  auto_l2_generation: true        # Hook generates L2 automatically

# V2.1.10 Features (NEW)
v2110_features:
  structured_prompts: true         # PromptTemplateBuilder with JSON Schema
  l2_synthesizer: true             # Multi-source synthesis for comprehensive clarification
  json_output_validation: true     # AskOutput schema validation via Hook
  output_schema: ask               # Points to AskOutput in output_schemas.py
---

# /ask Command (V2.1.10 Enhanced)

$ARGUMENTS

프롬프트 어시스턴트를 호출하여:
1. **요구사항 분석** - 사용자 의도 파악 및 명확도 평가
2. **Socratic Questioning** - 모호한 부분 명확화 (clarity_score < 0.7일 때)
3. **기능 추천** - 적합한 Claude Code Native Capabilities 제안
4. **프롬프트 개선** - 더 효과적인 프롬프트 구조 제안
5. **사용자 승인 후 실행**

---

## Target User Profile

| 항목 | 설명 |
|------|------|
| **레벨** | 중급 사용자 (기본 기능은 알지만 최적의 접근법을 찾는 사용자) |
| **핵심 가치** | 명확화 + 기능 추천 + 프롬프트 개선 **통합** |
| **기대 결과** | 구체적이고 실행 가능한 다음 단계 제시 |

---

## Orchestration Pattern (V2.1.10)

```
┌─────────────────────────────────────────────────────────────────────┐
│                    V2.1.10 ORCHESTRATION FLOW                       │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│  ┌─────────────┐   Context    ┌──────────────────┐                 │
│  │  Main Agent │───Budget ───►│ ContextBudget    │                 │
│  │ (Conductor) │   Check      │   Manager        │                 │
│  └──────┬──────┘              └──────────────────┘                 │
│         │                                                           │
│         │ Parallel Delegation (run_in_background=True)             │
│         ▼                                                           │
│  ┌──────────────────────────────────────────────────────────┐      │
│  │                    PARALLEL SOURCES                      │      │
│  │                                                          │      │
│  │  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐   │      │
│  │  │ prompt-      │  │ claude-code- │  │  WebSearch   │   │      │
│  │  │ assistant    │  │ guide        │  │  (latest     │   │      │
│  │  │ (Clarify)    │  │ (Docs)       │  │  techniques) │   │      │
│  │  └──────┬───────┘  └──────┬───────┘  └──────┬───────┘   │      │
│  │         │                 │                 │            │      │
│  │         └────────────┬────┴────────────────┘            │      │
│  │                      ▼                                   │      │
│  │              L2 Files Generated                          │      │
│  └──────────────────────────────────────────────────────────┘      │
│                      │                                              │
│                      ▼                                              │
│              ┌──────────────────┐                                   │
│              │  L2Synthesizer   │  ~500 tokens                     │
│              │  (Condense)      │  essential context               │
│              └────────┬─────────┘                                   │
│                       │                                             │
│                       ▼                                             │
│              ┌──────────────────┐                                   │
│              │ AskUserQuestion  │  Present recommendations         │
│              │ (if needed)      │  to user                         │
│              └──────────────────┘                                   │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
```

**Key Principles:**
- **Context Budget Check FIRST** - Verify context before any delegation
- **Parallel Multi-Source** - WebSearch + claude-code-guide + prompt-assistant 병렬 실행
- **L2Synthesizer** - 다중 소스 결과를 ~500 tokens로 압축
- **Structured Output** - AskOutput 스키마로 일관된 JSON 출력

---

## Execution Strategy (V2.1.10)

### Step 0: Context Budget Check (MANDATORY)

```python
from lib.oda.planning.context_budget_manager import (
    ContextBudgetManager,
    ThinkingMode,
    DelegationDecision,
)

# Check context before delegation
manager = ContextBudgetManager(thinking_mode=ThinkingMode.STANDARD)
decision = manager.check_before_delegation(
    subagent_type="prompt-assistant",
    estimated_tokens=8000
)

if decision == DelegationDecision.PROCEED:
    # Safe to delegate - proceed with parallel execution
    pass
elif decision == DelegationDecision.REDUCE_SCOPE:
    # Reduce to single source (prompt-assistant only)
    pass
elif decision in [DelegationDecision.DEFER, DelegationDecision.ABORT]:
    # Suggest /compact first
    print("⚠️ Context usage high. Consider running /compact first.")
```

### Step 1: Parallel Multi-Source Information Gathering

Deploy multiple sources in parallel for comprehensive clarification:

```python
from lib.oda.planning.prompt_templates import build_structured_prompt

# V2.1.10: Build structured prompt with JSON Schema
structured_prompt = build_structured_prompt(
    subagent_type="ask",  # Uses AskOutput schema
    task_description=f"""Analyze and clarify user request: {user_input}

Target User: Intermediate Claude Code user
Core Values: Clarification + Skill Recommendation + Prompt Improvement

Provide:
1. Clarity score (0.0-1.0)
2. Intent analysis
3. Clarification questions (if clarity < 0.7)
4. Skill recommendations with confidence
5. Prompt improvement suggestions""",
    budget=8000,
    additional_context="Focus on actionable next steps"
)

# Source 1: prompt-assistant (main clarification)
Task(
    subagent_type="prompt-assistant",
    prompt=structured_prompt,  # JSON Schema embedded
    description="Clarify user requirements",
    run_in_background=True
)

# Source 2: claude-code-guide (documentation)
Task(
    subagent_type="claude-code-guide",
    prompt=f"""
    User query: {user_input}

    Search for:
    1. Relevant Claude Code features
    2. Best practices for this use case
    3. Example invocations

    Return structured recommendations.
    """,
    description="Search official documentation",
    run_in_background=True
)

# Source 3: WebSearch (if latest techniques needed)
if any(kw in user_input.lower() for kw in ["최신", "2025", "2026", "best practice", "모범 사례"]):
    WebSearch(query=f"Claude Code prompt engineering best practices 2026 {user_input}")
```

### Step 2: L2 Synthesis (Multi-Source Condensation)

After parallel sources complete, synthesize results:

```python
from lib.oda.planning.l2_synthesizer import L2Synthesizer

# Collect L2 paths from completed agents
l2_paths = [
    f".agent/outputs/ask/{prompt_assistant_agent_id}_structured.json",
    f".agent/outputs/claude-code-guide/{guide_agent_id}_structured.md",
]

# V2.1.10: Synthesize multi-source results
synthesizer = L2Synthesizer()
synthesis_result = synthesizer.synthesize(
    l2_paths=l2_paths,
    synthesis_goal="comprehensive user guidance with prioritized recommendations",
    max_output_tokens=500
)

# Synthesis result contains:
# - Merged clarity assessment
# - Prioritized clarification questions
# - Consolidated skill recommendations
# - Best prompt improvement suggestion
```

### Step 3: Present to User with AskUserQuestion

```python
# If clarity_score < 0.7, present clarification questions
if synthesis_result.clarity_score < 0.7:
    AskUserQuestion(
        questions=[
            {
                "question": q.question,
                "header": q.category.capitalize(),
                "options": generate_options_for_category(q.category),
                "multiSelect": False
            }
            for q in synthesis_result.questions[:3]  # Top 3 priority questions
        ]
    )
else:
    # High clarity - recommend skill directly
    recommended_skill = synthesis_result.skill_recommendations[0]
    Skill(skill=recommended_skill.skill_name, args=user_input)
```

---

## Output Schema (V2.1.10)

`/ask` uses `AskOutput` schema from `lib/oda/planning/output_schemas.py`:

```python
class AskOutput(BaseModel):
    clarity_score: float           # 0.0-1.0, clarity of user request
    summary: str                   # One-sentence intent summary
    intent_analysis: str           # What user is trying to accomplish
    questions: List[ClarificationQuestion]  # Socratic questions
    skill_recommendations: List[SkillRecommendation]  # Tool suggestions
    prompt_improvement: Optional[str]  # Better prompt structure
    techniques_applied: List[str]  # Prompting techniques used
    referenced_documentation: List[str]  # Relevant doc paths
    next_action: str               # What user should do next
```

**Schema Benefits:**
- Hook automatically validates output
- L2Synthesizer can parse efficiently
- Multi-ask workflows can aggregate results
- Recovery after Auto-Compact is seamless

---

## Context Cost Comparison

| Scenario | Without V2.1.10 | With V2.1.10 |
|----------|-----------------|--------------|
| 3 parallel sources (raw) | ~15K tokens | ~300 tokens (L1 headlines) |
| L2 access (if needed) | ~15K tokens | 0 (Synthesizer reads) |
| Synthesis result | N/A | ~500 tokens |
| **Total Context Cost** | **~30K tokens** | **~800 tokens** |
| **Reduction** | - | **~97%** |

---

## Clarification Workflow

```
User: "이거 좀 봐줘" (Look at this)

┌─────────────────────────────────────────┐
│ Step 1: Analyze Intent                  │
│ - clarity_score: 0.3 (very unclear)     │
│ - intent: "Code review requested"       │
│ - missing: scope, focus area, goal      │
└────────────────┬────────────────────────┘
                 │
                 ▼
┌─────────────────────────────────────────┐
│ Step 2: Generate Questions              │
│ Q1: "어떤 파일을 검토할까요?" (scope)    │
│ Q2: "버그, 성능, 스타일 중 무엇?" (intent)│
│ Q3: "특별히 걱정되는 부분이 있나요?"      │
└────────────────┬────────────────────────┘
                 │
                 ▼
┌─────────────────────────────────────────┐
│ Step 3: AskUserQuestion                 │
│ Present questions with options          │
│ Wait for user response                  │
└────────────────┬────────────────────────┘
                 │
                 ▼
┌─────────────────────────────────────────┐
│ Step 4: Route to Appropriate Skill      │
│ User answers → clarity_score: 0.9       │
│ Recommend: /audit src/auth.py           │
└─────────────────────────────────────────┘
```

---

## Integration with V4.0 LLM-Native Routing

When Main Agent receives ambiguous natural language, auto-route to `/ask`:

```python
# In Main Agent behavior
AMBIGUITY_KEYWORDS = [
    "어떻게", "뭐가", "뭘", "모르겠", "도와줘",
    "how", "what", "help", "unclear", "confused"
]

if any(kw in user_message.lower() for kw in AMBIGUITY_KEYWORDS):
    # Auto-invoke /ask without explicit command
    Skill(skill="ask", args=user_message)
```

---

## Error Handling

| 상황 | 처리 방법 |
|------|----------|
| prompt-assistant 로드 실패 | WebSearch + claude-code-guide만으로 진행 |
| Context budget 부족 | 단일 소스(prompt-assistant)로 축소 |
| L2 synthesis 실패 | L2 파일 직접 읽기 fallback |
| clarity_score 계산 불가 | 기본값 0.5로 설정, 질문 생성 진행 |

---

## Example Usage

```bash
# 모호한 요청
/ask 이거 좀 고쳐줘
# → Socratic questioning → clarified request → /audit or /plan

# 명확한 요청
/ask src/auth.py에서 보안 취약점 찾아줘
# → clarity_score: 0.9 → recommend /deep-audit src/auth.py

# 프롬프트 개선 요청
/ask 좋은 시스템 프롬프트 작성법 알려줘
# → WebSearch + claude-code-guide → prompt engineering techniques
```

---

## References

- **Output Schema:** `lib/oda/planning/output_schemas.py` (AskOutput)
- **Prompt Templates:** `lib/oda/planning/prompt_templates.py`
- **L2Synthesizer:** `lib/oda/planning/l2_synthesizer.py`
- **Agent Definition:** `.claude/agents/prompt-assistant.md`
